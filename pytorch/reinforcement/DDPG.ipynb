{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzhs/243z+dzD1q+948iV7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimamt/machine_learning/blob/master/pytorch/reinforcement/DDPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LBKqi1R1bje",
        "outputId": "57d7ed72-3924-42f0-b974-31b0c434e24c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Score = -1495.8096513071378\n",
            "Episode 2: Score = -1279.6137613944802\n",
            "Episode 3: Score = -1515.6333502003433\n",
            "Episode 4: Score = -1543.9457278028076\n",
            "Episode 5: Score = -1618.4753614240276\n",
            "Episode 6: Score = -1704.3358516089615\n",
            "Episode 7: Score = -1588.612132187681\n",
            "Episode 8: Score = -1520.0003493706076\n",
            "Episode 9: Score = -1070.093247273965\n",
            "Episode 10: Score = -1612.3368379793503\n",
            "Episode 11: Score = -1454.891339762761\n",
            "Episode 12: Score = -1513.9859745340004\n",
            "Episode 13: Score = -1369.7262120004164\n",
            "Episode 14: Score = -1317.9358210519536\n",
            "Episode 15: Score = -1493.9071086809915\n",
            "Episode 16: Score = -1666.9485635450824\n",
            "Episode 17: Score = -1496.787415449485\n",
            "Episode 18: Score = -1393.0399263172287\n",
            "Episode 19: Score = -1490.9244241214496\n",
            "Episode 20: Score = -1643.5984534748125\n",
            "Episode 21: Score = -1618.1295054829827\n",
            "Episode 22: Score = -1325.6224166056031\n",
            "Episode 23: Score = -1456.4951653731598\n",
            "Episode 24: Score = -1423.1960840699808\n",
            "Episode 25: Score = -1480.8188341069406\n",
            "Episode 26: Score = -1566.1286124986132\n",
            "Episode 27: Score = -1643.8098102734648\n",
            "Episode 28: Score = -1495.1983208933652\n",
            "Episode 29: Score = -1635.2968344373942\n",
            "Episode 30: Score = -1149.1230978287183\n",
            "Episode 31: Score = -1610.749625107019\n",
            "Episode 32: Score = -1493.5023350328172\n",
            "Episode 33: Score = -1237.5812916230173\n",
            "Episode 34: Score = -1594.6392627205332\n",
            "Episode 35: Score = -1158.4452885711894\n",
            "Episode 36: Score = -1662.9600607141758\n",
            "Episode 37: Score = -1166.561294206867\n",
            "Episode 38: Score = -1624.8796958323414\n",
            "Episode 39: Score = -1615.2476443280007\n",
            "Episode 40: Score = -1131.5174493919342\n",
            "Episode 41: Score = -1157.5693901638545\n",
            "Episode 42: Score = -1122.2024526769653\n",
            "Episode 43: Score = -1117.5672327783348\n",
            "Episode 44: Score = -1495.5332347098033\n",
            "Episode 45: Score = -1660.2387864738373\n",
            "Episode 46: Score = -1088.8693432588734\n",
            "Episode 47: Score = -1670.4766183400188\n",
            "Episode 48: Score = -1578.9451949978716\n",
            "Episode 49: Score = -1685.9042301957177\n",
            "Episode 50: Score = -1651.0250804957282\n",
            "Episode 51: Score = -1245.1154840497986\n",
            "Episode 52: Score = -1609.2158695860126\n",
            "Episode 53: Score = -1373.0954755305054\n",
            "Episode 54: Score = -1364.5751321852542\n",
            "Episode 55: Score = -1464.4430894137033\n",
            "Episode 56: Score = -1496.7339950813705\n",
            "Episode 57: Score = -1465.909582541461\n",
            "Episode 58: Score = -1498.493730418684\n",
            "Episode 59: Score = -1557.593496814768\n",
            "Episode 60: Score = -1571.5760974606621\n",
            "Episode 61: Score = -1543.2364270873638\n",
            "Episode 62: Score = -1519.3692291020714\n",
            "Episode 63: Score = -1549.6841303837807\n",
            "Episode 64: Score = -1329.5112882844633\n",
            "Episode 65: Score = -1562.4435425625775\n",
            "Episode 66: Score = -1571.3364682945\n",
            "Episode 67: Score = -1475.4474437261092\n",
            "Episode 68: Score = -1552.2233123481014\n",
            "Episode 69: Score = -1428.3427320708422\n",
            "Episode 70: Score = -1422.2818170194257\n",
            "Episode 71: Score = -1335.4028917595942\n",
            "Episode 72: Score = -1568.6937668930552\n",
            "Episode 73: Score = -1608.2752006444236\n",
            "Episode 74: Score = -1623.8195434073984\n",
            "Episode 75: Score = -1491.2227374221568\n",
            "Episode 76: Score = -1609.5230419350016\n",
            "Episode 77: Score = -1504.8804672125748\n",
            "Episode 78: Score = -1438.0681668275602\n",
            "Episode 79: Score = -1435.258323267397\n",
            "Episode 80: Score = -1423.9797408176523\n",
            "Episode 81: Score = -1527.2277169918898\n",
            "Episode 82: Score = -1567.1213754162177\n",
            "Episode 83: Score = -550.3758682842101\n",
            "Episode 84: Score = -937.183520194056\n",
            "Episode 85: Score = -1577.1182027519799\n",
            "Episode 86: Score = -1642.3011423687249\n",
            "Episode 87: Score = -792.5162490216413\n",
            "Episode 88: Score = -1630.122990671014\n",
            "Episode 89: Score = -1630.1683011015568\n",
            "Episode 90: Score = -1121.1057882106963\n",
            "Episode 91: Score = -1221.120241468256\n",
            "Episode 92: Score = -1490.9766380030617\n",
            "Episode 93: Score = -1244.1788759176497\n",
            "Episode 94: Score = -1264.41731284311\n",
            "Episode 95: Score = -1361.5023488119252\n",
            "Episode 96: Score = -1578.4844012554345\n",
            "Episode 97: Score = -1570.8738919731877\n",
            "Episode 98: Score = -1143.6685831994723\n",
            "Episode 99: Score = -1451.1262831623358\n",
            "Episode 100: Score = -1223.103103160123\n",
            "Episode 101: Score = -1579.2401154005365\n",
            "Episode 102: Score = -1616.2050012431328\n",
            "Episode 103: Score = -1203.073007671452\n",
            "Episode 104: Score = -1650.195095354433\n",
            "Episode 105: Score = -1593.9823255472247\n",
            "Episode 106: Score = -1595.6706621893975\n",
            "Episode 107: Score = -1587.365515223573\n",
            "Episode 108: Score = -131.00962938607682\n",
            "Episode 109: Score = -1518.1307941297614\n",
            "Episode 110: Score = -1095.7980621081686\n",
            "Episode 111: Score = -1392.185433569428\n",
            "Episode 112: Score = -1160.1393986642283\n",
            "Episode 113: Score = -1193.629476594336\n",
            "Episode 114: Score = -1050.9535540721863\n",
            "Episode 115: Score = -249.3026637920946\n",
            "Episode 116: Score = -247.0396977044934\n",
            "Episode 117: Score = -3.548446563015763\n",
            "Episode 118: Score = -128.1236856371613\n",
            "Episode 119: Score = -247.32196632579527\n",
            "Episode 120: Score = -130.49622085299288\n",
            "Episode 121: Score = -122.5628248487844\n",
            "Episode 122: Score = -626.6144297365414\n",
            "Episode 123: Score = -2.047759770959529\n",
            "Episode 124: Score = -1.1937986108887568\n",
            "Episode 125: Score = -1.0285753811744316\n",
            "Episode 126: Score = -125.99684530672596\n",
            "Episode 127: Score = -234.25735518019465\n",
            "Episode 128: Score = -231.4896491919155\n",
            "Episode 129: Score = -3.047289901409171\n",
            "Episode 130: Score = -129.53842270263112\n",
            "Episode 131: Score = -124.24993795386993\n",
            "Episode 132: Score = -257.6549614272143\n",
            "Episode 133: Score = -128.19982393127688\n",
            "Episode 134: Score = -138.81717395172424\n",
            "Episode 135: Score = -121.23891425526514\n",
            "Episode 136: Score = -344.6605823063929\n",
            "Episode 137: Score = -133.2530768047902\n",
            "Episode 138: Score = -129.14337269589828\n",
            "Episode 139: Score = -265.6948770586965\n",
            "Episode 140: Score = -6.5043254706463784\n",
            "Episode 141: Score = -5.887197113707055\n",
            "Episode 142: Score = -129.95008323948508\n",
            "Episode 143: Score = -132.9583040362203\n",
            "Episode 144: Score = -6.904659467766918\n",
            "Episode 145: Score = -6.436203399894863\n",
            "Episode 146: Score = -258.10237169883067\n",
            "Episode 147: Score = -126.3625177067258\n",
            "Episode 148: Score = -259.519462481349\n",
            "Episode 149: Score = -266.209771162631\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 256\n",
        "LR_ACTOR = 1e-4\n",
        "LR_CRITIC = 1e-3\n",
        "GAMMA = 0.99\n",
        "TAU = 1e-3\n",
        "BUFFER_SIZE = int(1e6)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ActorNet(torch.nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(ActorNet, self).__init__()\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(state_size, 256, device=device)\n",
        "        self.fc2 = torch.nn.Linear(256, 128, device=device)\n",
        "        self.fc3 = torch.nn.Linear(128, action_size, device=device)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        x = self.fc1_init()\n",
        "        self.fc1.weight.data.uniform_(x[0],x[1])\n",
        "        x = self.fc2_init()\n",
        "        self.fc2.weight.data.uniform_(x[0],x[1])\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def fc1_init(self):\n",
        "        lim = 1. / np.sqrt(self.fc1.weight.data.size()[0])\n",
        "        return (-lim, lim)\n",
        "\n",
        "    def fc2_init(self):\n",
        "        lim = 1. / np.sqrt(self.fc2.weight.data.size()[0])\n",
        "        return (-lim, lim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.tanh(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class CriticNet(torch.nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(CriticNet, self).__init__()\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(state_size + action_size, 256, device=device)\n",
        "        self.fc2 = torch.nn.Linear(256, 128, device=device)\n",
        "        self.fc3 = torch.nn.Linear(128, 1, device=device)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fc1.weight.data.uniform_(*self.fc1_init())\n",
        "        self.fc2.weight.data.uniform_(*self.fc2_init())\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def fc1_init(self):\n",
        "        lim = 1. / np.sqrt(self.fc1.weight.data.size()[0])\n",
        "        return (-lim, lim)\n",
        "\n",
        "    def fc2_init(self):\n",
        "        lim = 1. / np.sqrt(self.fc2.weight.data.size()[0])\n",
        "        return (-lim, lim)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.cat([state, action], 1).to(device)\n",
        "        x = F.relu(self.fc1(state_action))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DDPG:\n",
        "    def __init__(self, state_dim, action_dim, action_high):\n",
        "        self.actor = ActorNet(state_dim, action_dim)\n",
        "        self.actor_target = ActorNet(state_dim, action_dim)\n",
        "        self.critic = CriticNet(state_dim, action_dim)\n",
        "        self.critic_target = CriticNet(state_dim, action_dim)\n",
        "        self.memory = deque(maxlen=BUFFER_SIZE)\n",
        "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
        "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
        "        self.action_high = action_high\n",
        "\n",
        "    def act(self, state):\n",
        "        state = Variable(torch.from_numpy(state).float().to(device).unsqueeze(0))\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor(state.to(device))\n",
        "        self.actor.train()\n",
        "        return np.clip(action.cpu().numpy()[0] * self.action_high, -self.action_high, self.action_high)\n",
        "\n",
        "    def memorize(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, BATCH_SIZE)\n",
        "        state_batch = torch.from_numpy(np.array([arr[0] for arr in batch])).float().to(device)\n",
        "        action_batch = torch.from_numpy(np.array([arr[1] for arr in batch])).float().to(device)\n",
        "        reward_batch = torch.from_numpy(np.array([arr[2] for arr in batch]).reshape(BATCH_SIZE, 1)).float().to(device)\n",
        "        next_state_batch = torch.from_numpy(np.array([arr[3] for arr in batch])).float().to(device)\n",
        "        done_batch = torch.from_numpy(np.array([arr[4] for arr in batch], dtype=np.uint8).reshape(BATCH_SIZE, 1)).float().to(device)\n",
        "\n",
        "        next_actions = self.actor_target(next_state_batch)\n",
        "        q_next = self.critic_target(next_state_batch, next_actions)\n",
        "        q_targets = reward_batch + GAMMA * q_next * (1 - done_batch)\n",
        "\n",
        "        # Update critic\n",
        "        self.optimizer_critic.zero_grad()\n",
        "        q_current = self.critic(state_batch, action_batch)\n",
        "        critic_loss = F.mse_loss(q_current, q_targets)\n",
        "        critic_loss.backward()\n",
        "        self.optimizer_critic.step()\n",
        "\n",
        "        # Update actor\n",
        "        self.optimizer_actor.zero_grad()\n",
        "        actor_loss = -self.critic(state_batch, self.actor(state_batch)).mean()\n",
        "        actor_loss.backward()\n",
        "        self.optimizer_actor.step()\n",
        "\n",
        "        # Update target networks\n",
        "        self.update_targets()\n",
        "\n",
        "    def update_targets(self):\n",
        "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
        "            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
        "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
        "            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"Pendulum-v1\")\n",
        "    agent = DDPG(env.observation_space.shape[0], env.action_space.shape[0], env.action_space.high[0])\n",
        "    scores = []\n",
        "    for i_episode in range(1, 150):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(1000):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            agent.memorize(state, action, reward, next_state, done)\n",
        "            agent.learn()\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "        scores.append(score)\n",
        "        print(\"Episode {}: Score = {}\".format(i_episode, score))"
      ]
    }
  ]
}