{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimamt/machine_learning/blob/master/pytorch/reinforcement/TD3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnjG29B4oDgK",
        "outputId": "c738d50f-b644-47cc-84fa-208f68198077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: Score = -915.4854227131614\n",
            "Episode 2: Score = -1371.2139301632176\n",
            "Episode 3: Score = -1321.0586690929742\n",
            "Episode 4: Score = -944.6033472877201\n",
            "Episode 5: Score = -1601.682867510128\n",
            "Episode 6: Score = -1517.3193491452625\n",
            "Episode 7: Score = -1669.9879787643602\n",
            "Episode 8: Score = -1678.0348837244017\n",
            "Episode 9: Score = -1826.342537554729\n",
            "Episode 10: Score = -1831.9836459000537\n",
            "Episode 11: Score = -1599.1568174339661\n",
            "Episode 12: Score = -1225.8292465213945\n",
            "Episode 13: Score = -1609.0338192872075\n",
            "Episode 14: Score = -1399.749496148025\n",
            "Episode 15: Score = -1382.6626497437396\n",
            "Episode 16: Score = -1353.101447095879\n",
            "Episode 17: Score = -1550.1940531328553\n",
            "Episode 18: Score = -1555.4553992842655\n",
            "Episode 19: Score = -1433.084477257876\n",
            "Episode 20: Score = -1479.9490777096285\n",
            "Episode 21: Score = -1286.4736544913544\n",
            "Episode 22: Score = -1396.6915768863096\n",
            "Episode 23: Score = -1379.4575795217206\n",
            "Episode 24: Score = -1211.79418358925\n",
            "Episode 25: Score = -1476.2376071970546\n",
            "Episode 26: Score = -1347.1206023551185\n",
            "Episode 27: Score = -1007.9999017845215\n",
            "Episode 28: Score = -1442.9510825010889\n",
            "Episode 29: Score = -1498.0262306645964\n",
            "Episode 30: Score = -1497.4209301205774\n",
            "Episode 31: Score = -1341.1146726910413\n",
            "Episode 32: Score = -1475.8057505808304\n",
            "Episode 33: Score = -1502.6754786906877\n",
            "Episode 34: Score = -642.8053651872426\n",
            "Episode 35: Score = -1495.4991977358106\n",
            "Episode 36: Score = -1460.1747793946545\n",
            "Episode 37: Score = -1393.786439245221\n",
            "Episode 38: Score = -1363.4034866488141\n",
            "Episode 39: Score = -1271.9189295100875\n",
            "Episode 40: Score = -1351.264684338681\n",
            "Episode 41: Score = -1465.3756456972596\n",
            "Episode 42: Score = -1004.9225329576686\n",
            "Episode 43: Score = -946.5703583808921\n",
            "Episode 44: Score = -1537.9248954924096\n",
            "Episode 45: Score = -1135.203082177112\n",
            "Episode 46: Score = -1527.9905973097311\n",
            "Episode 47: Score = -1501.4707361152527\n",
            "Episode 48: Score = -1495.1248320032132\n",
            "Episode 49: Score = -1418.9273075111446\n",
            "Episode 50: Score = -1145.0792220902754\n",
            "Episode 51: Score = -1442.0230219121968\n",
            "Episode 52: Score = -1067.634637075535\n",
            "Episode 53: Score = -1314.8142895661647\n",
            "Episode 54: Score = -1530.4468914667489\n",
            "Episode 55: Score = -1517.012542661554\n",
            "Episode 56: Score = -1164.140102207611\n",
            "Episode 57: Score = -1005.1800116526808\n",
            "Episode 58: Score = -1494.3983565413275\n",
            "Episode 59: Score = -1231.4533614450331\n",
            "Episode 60: Score = -1432.444310949872\n",
            "Episode 61: Score = -1405.2790458008415\n",
            "Episode 62: Score = -1461.9394456454565\n",
            "Episode 63: Score = -1364.3155463249618\n",
            "Episode 64: Score = -1490.9443390621364\n",
            "Episode 65: Score = -1515.3317516523286\n",
            "Episode 66: Score = -1494.217002860163\n",
            "Episode 67: Score = -1521.4882406787092\n",
            "Episode 68: Score = -1525.707987170402\n",
            "Episode 69: Score = -991.5288403176569\n",
            "Episode 70: Score = -978.6575404844709\n",
            "Episode 71: Score = -1491.503246623606\n",
            "Episode 72: Score = -809.3600307739392\n",
            "Episode 73: Score = -1491.8423283119873\n",
            "Episode 74: Score = -1101.500840583224\n",
            "Episode 75: Score = -1392.7827220485053\n",
            "Episode 76: Score = -1295.6275152685998\n",
            "Episode 77: Score = -1432.9859081214447\n",
            "Episode 78: Score = -1374.3695058674543\n",
            "Episode 79: Score = -1422.9878658014948\n",
            "Episode 80: Score = -1482.163538219684\n",
            "Episode 81: Score = -1444.670502044547\n",
            "Episode 82: Score = -962.3079484005165\n",
            "Episode 83: Score = -805.1217664581089\n",
            "Episode 84: Score = -891.8468925370595\n",
            "Episode 85: Score = -985.8849613172663\n",
            "Episode 86: Score = -797.297467246619\n",
            "Episode 87: Score = -778.4150520965281\n",
            "Episode 88: Score = -1494.1330933382922\n",
            "Episode 89: Score = -853.3987837683146\n",
            "Episode 90: Score = -7.5195399421936715\n",
            "Episode 91: Score = -11.012054146137508\n",
            "Episode 92: Score = -639.5044009182033\n",
            "Episode 93: Score = -523.749370674429\n",
            "Episode 94: Score = -422.0994104776814\n",
            "Episode 95: Score = -143.81113205653494\n",
            "Episode 96: Score = -142.23128001392607\n",
            "Episode 97: Score = -1492.598938339409\n",
            "Episode 98: Score = -270.0920425485801\n",
            "Episode 99: Score = -265.3417254376255\n",
            "Episode 100: Score = -575.1336315998926\n",
            "Episode 101: Score = -263.05321380314007\n",
            "Episode 102: Score = -265.90548152695305\n",
            "Episode 103: Score = -404.31894887594996\n",
            "Episode 104: Score = -446.8573209572225\n",
            "Episode 105: Score = -414.60326917288546\n",
            "Episode 106: Score = -420.1725409594317\n",
            "Episode 107: Score = -543.0155860116814\n",
            "Episode 108: Score = -263.56374774344914\n",
            "Episode 109: Score = -1496.3403601351172\n",
            "Episode 110: Score = -262.8400969783014\n",
            "Episode 111: Score = -658.4736613914052\n",
            "Episode 112: Score = -264.15099531871476\n",
            "Episode 113: Score = -396.4658471947782\n",
            "Episode 114: Score = -261.67049246639\n",
            "Episode 115: Score = -417.90937073031307\n",
            "Episode 116: Score = -139.35886268687133\n",
            "Episode 117: Score = -401.2767546795386\n",
            "Episode 118: Score = -263.9892342202939\n",
            "Episode 119: Score = -259.65403946276774\n",
            "Episode 120: Score = -1490.9271229625906\n",
            "Episode 121: Score = -6.465701100091173\n",
            "Episode 122: Score = -259.46920660566224\n",
            "Episode 123: Score = -574.265522370016\n",
            "Episode 124: Score = -129.0409055907438\n",
            "Episode 125: Score = -443.584728107399\n",
            "Episode 126: Score = -266.6945180797788\n",
            "Episode 127: Score = -2.3759773336503573\n",
            "Episode 128: Score = -258.60700292877493\n",
            "Episode 129: Score = -137.06910068400074\n",
            "Episode 130: Score = -264.4997501665777\n",
            "Episode 131: Score = -125.47947236614623\n",
            "Episode 132: Score = -366.2960035033932\n",
            "Episode 133: Score = -132.7961838224845\n",
            "Episode 134: Score = -127.03691484424267\n",
            "Episode 135: Score = -131.7235020966278\n",
            "Episode 136: Score = -126.26552191534682\n",
            "Episode 137: Score = -410.3883245568337\n",
            "Episode 138: Score = -121.8347739679053\n",
            "Episode 139: Score = -1.12093134912177\n",
            "Episode 140: Score = -127.4525136990039\n",
            "Episode 141: Score = -129.53599631640878\n",
            "Episode 142: Score = -274.8699306482918\n",
            "Episode 143: Score = -1.8741020408296527\n",
            "Episode 144: Score = -276.853249067724\n",
            "Episode 145: Score = -133.6849876746401\n",
            "Episode 146: Score = -129.51346998175194\n",
            "Episode 147: Score = -120.24382182683138\n",
            "Episode 148: Score = -131.7627728000403\n",
            "Episode 149: Score = -1491.151042033998\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 256\n",
        "LR_ACTOR = 1e-4\n",
        "LR_CRITIC = 1e-3\n",
        "STD_NOISE = 0.3\n",
        "GAMMA = 0.99\n",
        "TAU = 1e-3\n",
        "BUFFER_SIZE = int(1e6)\n",
        "STEPS = 1000\n",
        "POLICY_NOISE = 0.2\n",
        "NOISE_CLIP = 0.5\n",
        "DELAY_STEPS = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ActorNet(torch.nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(ActorNet, self).__init__()\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(state_size, 256, device=device)\n",
        "        self.fc2 = torch.nn.Linear(256, 128, device=device)\n",
        "        self.fc3 = torch.nn.Linear(128, action_size, device=device)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        x = self.fc1_init()\n",
        "        self.fc1.weight.data.uniform_(x[0],x[1])\n",
        "        x = self.fc2_init()\n",
        "        self.fc2.weight.data.uniform_(x[0],x[1])\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def fc1_init(self):\n",
        "        lim = 1. / np.sqrt(self.fc1.weight.data.size()[0])\n",
        "        return (-lim, lim)\n",
        "\n",
        "    def fc2_init(self):\n",
        "        lim = 1. / np.sqrt(self.fc2.weight.data.size()[0])\n",
        "        return (-lim, lim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.tanh(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class CriticNet(torch.nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(CriticNet, self).__init__()\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(state_size + action_size, 256, device=device)\n",
        "        self.fc2 = torch.nn.Linear(256, 128, device=device)\n",
        "        self.fc3 = torch.nn.Linear(128, 1, device=device)\n",
        "\n",
        "        self.fc4 = torch.nn.Linear(state_size + action_size, 256, device=device)\n",
        "        self.fc5 = torch.nn.Linear(256, 128, device=device)\n",
        "        self.fc6 = torch.nn.Linear(128, 1, device=device)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fc1.weight.data.uniform_(*self.fc1_init())\n",
        "        self.fc2.weight.data.uniform_(*self.fc2_init())\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "        self.fc4.weight.data.uniform_(*self.fc1_init())\n",
        "        self.fc5.weight.data.uniform_(*self.fc2_init())\n",
        "        self.fc6.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def fc1_init(self):\n",
        "        lim = 1. / np.sqrt(self.fc1.weight.data.size()[0])\n",
        "        return (-lim, lim)\n",
        "\n",
        "    def fc2_init(self):\n",
        "        lim = 1. / np.sqrt(self.fc2.weight.data.size()[0])\n",
        "        return (-lim, lim)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.cat([state, action], 1).to(device)\n",
        "        x = F.relu(self.fc1(state_action))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        y = F.relu(self.fc4(state_action))\n",
        "        y = F.relu(self.fc5(y))\n",
        "        y = self.fc6(y)\n",
        "        return x,y\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "        state_action = torch.cat([state, action], 1).to(device)\n",
        "        x = F.relu(self.fc1(state_action))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class DDPG:\n",
        "    def __init__(self, state_dim, action_dim, action_high):\n",
        "        self.actor = ActorNet(state_dim, action_dim)\n",
        "        self.actor_target = ActorNet(state_dim, action_dim)\n",
        "        self.critic = CriticNet(state_dim, action_dim)\n",
        "        self.critic_target = CriticNet(state_dim, action_dim)\n",
        "        self.memory = deque(maxlen=BUFFER_SIZE)\n",
        "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
        "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
        "        self.action_high = action_high\n",
        "        self.step = 0\n",
        "\n",
        "    def act(self, state):\n",
        "        state = Variable(torch.from_numpy(state).float().to(device).unsqueeze(0))\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor(state.to(device))\n",
        "        self.actor.train()\n",
        "        return np.clip(action.cpu().numpy()[0] * self.action_high, -self.action_high, self.action_high)\n",
        "\n",
        "    def memorize(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        self.step += 1\n",
        "\n",
        "        batch = random.sample(self.memory, BATCH_SIZE)\n",
        "        state_batch = torch.from_numpy(np.array([arr[0] for arr in batch])).float().to(device)\n",
        "        action_batch = torch.from_numpy(np.array([arr[1] for arr in batch])).float().to(device)\n",
        "        reward_batch = torch.from_numpy(np.array([arr[2] for arr in batch]).reshape(BATCH_SIZE, 1)).float().to(device)\n",
        "        next_state_batch = torch.from_numpy(np.array([arr[3] for arr in batch])).float().to(device)\n",
        "        done_batch = torch.from_numpy(np.array([arr[4] for arr in batch], dtype=np.uint8).reshape(BATCH_SIZE, 1)).float().to(device)\n",
        "\n",
        "        next_actions = self.actor_target(next_state_batch)\n",
        "        # noise = torch.zeros_like(next_actions).to(device)\n",
        "        # noise.normal_(std=STD_NOISE)\n",
        "        # Paper code\n",
        "        noise = (\n",
        "\t\t\t\t  torch.randn_like(next_actions) * POLICY_NOISE\n",
        "\t\t\t  ).clamp(-NOISE_CLIP, NOISE_CLIP)\n",
        "        next_actions = (next_actions + noise).clamp(-self.action_high, self.action_high)\n",
        "        q_next1, q_next2 = self.critic_target(next_state_batch, next_actions)\n",
        "        q_targets = reward_batch + GAMMA * torch.min(q_next1,q_next2) * (1 - done_batch)\n",
        "\n",
        "        # Update critic\n",
        "        self.optimizer_critic.zero_grad()\n",
        "        q_current1, q_current2 = self.critic(state_batch, action_batch)\n",
        "        critic_loss = F.mse_loss(q_current1, q_targets) + F.mse_loss(q_current2, q_targets)\n",
        "        critic_loss.backward()\n",
        "        self.optimizer_critic.step()\n",
        "\n",
        "        if self.step % DELAY_STEPS == 0:\n",
        "          # Update actor\n",
        "          self.optimizer_actor.zero_grad()\n",
        "          actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n",
        "          actor_loss.backward()\n",
        "          self.optimizer_actor.step()\n",
        "\n",
        "          # Update target networks\n",
        "          self.update_targets()\n",
        "\n",
        "    def update_targets(self):\n",
        "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
        "            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
        "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
        "            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"Pendulum-v1\")\n",
        "    agent = DDPG(env.observation_space.shape[0], env.action_space.shape[0], env.action_space.high[0])\n",
        "    scores = []\n",
        "    for i_episode in range(1, 150):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(1000):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            agent.memorize(state, action, reward, next_state, done)\n",
        "            agent.learn()\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "        scores.append(score)\n",
        "        print(\"Episode {}: Score = {}\".format(i_episode, score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO1vh4Bqqm-4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcJFK0Pjk61+lTuwnQoatg",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}