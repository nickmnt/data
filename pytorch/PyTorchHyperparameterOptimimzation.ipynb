{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtu0C3vBwEX3WzkRIrkqtC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimamt/machine_learning/blob/master/pytorch/PyTorchHyperparameterOptimimzation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you will see, we only need to add some slight modifications. In particular, we need to\n",
        "\n",
        "    wrap data loading and training in functions,\n",
        "\n",
        "    make some network parameters configurable,\n",
        "\n",
        "    add checkpointing (optional),\n",
        "\n",
        "    and define the search space for the model tuning\n"
      ],
      "metadata": {
        "id": "wPZiWvBA8VUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ua3FHOk8m10",
        "outputId": "12be66e6-9a35-4639-8dd3-1c6e4cc6386c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ray\n",
            "  Downloading ray-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (59.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from ray) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.21.6)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray) (1.2.0)\n",
            "Collecting grpcio<=1.43.0,>=1.28.1\n",
            "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 14.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.4)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (22.1.0)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray) (2.23.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray) (1.3.1)\n",
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.16.5-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.0.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.10.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (3.0.4)\n",
            "Collecting distlib<1,>=0.3.5\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[K     |████████████████████████████████| 468 kB 62.0 MB/s \n",
            "\u001b[?25hCollecting platformdirs<3,>=2.4\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: platformdirs, distlib, virtualenv, grpcio, ray\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.49.1\n",
            "    Uninstalling grpcio-1.49.1:\n",
            "      Successfully uninstalled grpcio-1.49.1\n",
            "Successfully installed distlib-0.3.6 grpcio-1.43.0 platformdirs-2.5.2 ray-2.0.0 virtualenv-20.16.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nwnIOVeg77Vh"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_dir=\"./data\"):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=transform)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "    return trainset, testset"
      ],
      "metadata": {
        "id": "Crwsig1A8gAt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, l1=120, l2=84):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
        "        self.fc2 = nn.Linear(l1, l2)\n",
        "        self.fc3 = nn.Linear(l2, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hvwYEpm68hmm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wrap the training script in a function `train_cifar(config, checkpoint_dir=None, data_dir=None)`. As you can guess, the config parameter will receive the hyperparameters we would like to train with. The `checkpoint_dir` parameter is used to restore checkpoints. The `data_dir` specifies the directory where we load and store the data, so multiple runs can share the same data source."
      ],
      "metadata": {
        "id": "K1LGtErr9vJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cifar(config, checkpoint_dir=None, data_dir=None):\n",
        "    net = Net(config[\"l1\"], config[\"l2\"])\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            net = nn.DataParallel(net)\n",
        "    net.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        net.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    trainset, testset = load_data(data_dir)\n",
        "\n",
        "    test_abs = int(len(trainset) * 0.8)\n",
        "    train_subset, val_subset = random_split(\n",
        "        trainset, [test_abs, len(trainset) - test_abs])\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "\n",
        "    for epoch in range(10):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
        "                                                running_loss / epoch_steps))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validation loss\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.cpu().numpy()\n",
        "                val_steps += 1\n",
        "\n",
        "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
        "    print(\"Finished Training\")"
      ],
      "metadata": {
        "id": "cPnRYTSq8kKL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_accuracy(net, device=\"cpu\"):\n",
        "    trainset, testset = load_data()\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "0zCyQiPa-cDC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2**np.random.randint(2, 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB3RgCW7-pHH",
        "outputId": "b9946b83-9c8d-4f45-cbbb-518fd7d658c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
        "    \"l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
        "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "    \"batch_size\": tune.choice([2, 4, 8, 16])\n",
        "}"
      ],
      "metadata": {
        "id": "MF5UGhXI-jeO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    load_data(data_dir)\n",
        "    config = {\n",
        "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
        "    }\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "    result = tune.run(\n",
        "        partial(train_cifar, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "    print(\"Best trial final validation accuracy: {}\".format(\n",
        "        best_trial.last_result[\"accuracy\"]))\n",
        "\n",
        "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if gpus_per_trial > 1:\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    best_trained_model.to(device)\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "    test_acc = test_accuracy(best_trained_model, device)\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the number of GPUs per trial here:\n",
        "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bPtFJsFM-v2k",
        "outputId": "83cdc418-3ec1-4104-fcfb-6b8232f7bcc0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-10-16 13:35:01,312\tWARNING callback.py:109 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-10-16 13:35:01 (running for 00:00:00.22)\n",
            "Memory usage on this node: 2.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=694)\u001b[0m Files already downloaded and verified\n",
            "\u001b[2m\u001b[36m(func pid=694)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=694)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(func pid=694)\u001b[0m   cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-10-16 13:35:08 (running for 00:00:07.27)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:35:13 (running for 00:00:12.29)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:35:18 (running for 00:00:17.30)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=694)\u001b[0m [1,  2000] loss: 1.821\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:35:23 (running for 00:00:22.31)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:35:28 (running for 00:00:27.32)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+\n",
            "\n",
            "\n",
            "Result for train_cifar_55ab0_00000:\n",
            "  accuracy: 0.4265\n",
            "  date: 2022-10-16_13-35-29\n",
            "  done: false\n",
            "  experiment_id: 1b6f52b685f341e0b1617944a9a12771\n",
            "  hostname: c45a434aae96\n",
            "  iterations_since_restore: 1\n",
            "  loss: 1.5516036163330078\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 694\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 25.78715944290161\n",
            "  time_this_iter_s: 25.78715944290161\n",
            "  time_total_s: 25.78715944290161\n",
            "  timestamp: 1665927329\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 55ab0_00000\n",
            "  warmup_time: 0.006904125213623047\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:35:34 (running for 00:00:33.08)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |   loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.5516 |     0.4265 |                    1 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |        |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |        |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |        |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |        |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |        |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |        |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |        |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |        |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |        |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:35:39 (running for 00:00:38.09)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |   loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.5516 |     0.4265 |                    1 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |        |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |        |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |        |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |        |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |        |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |        |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |        |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |        |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |        |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:35:44 (running for 00:00:43.10)\n",
            "Memory usage on this node: 2.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |   loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.5516 |     0.4265 |                    1 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |        |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |        |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |        |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |        |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |        |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |        |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |        |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |        |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |        |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=694)\u001b[0m [2,  2000] loss: 1.513\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:35:49 (running for 00:00:48.11)\n",
            "Memory usage on this node: 2.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |   loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.5516 |     0.4265 |                    1 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |        |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |        |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |        |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |        |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |        |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |        |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |        |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |        |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |        |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:35:54 (running for 00:00:53.13)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |   loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.5516 |     0.4265 |                    1 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |        |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |        |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |        |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |        |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |        |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |        |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |        |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |        |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |        |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+--------+------------+----------------------+\n",
            "\n",
            "\n",
            "Result for train_cifar_55ab0_00000:\n",
            "  accuracy: 0.4905\n",
            "  date: 2022-10-16_13-35-54\n",
            "  done: false\n",
            "  experiment_id: 1b6f52b685f341e0b1617944a9a12771\n",
            "  hostname: c45a434aae96\n",
            "  iterations_since_restore: 2\n",
            "  loss: 1.412041555404663\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 694\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 51.280078649520874\n",
            "  time_this_iter_s: 25.492919206619263\n",
            "  time_total_s: 51.280078649520874\n",
            "  timestamp: 1665927354\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: 55ab0_00000\n",
            "  warmup_time: 0.006904125213623047\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:35:59 (running for 00:00:58.56)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.41204 |     0.4905 |                    2 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:36:04 (running for 00:01:03.58)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.41204 |     0.4905 |                    2 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:36:09 (running for 00:01:08.59)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.41204 |     0.4905 |                    2 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=694)\u001b[0m [3,  2000] loss: 1.430\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:36:14 (running for 00:01:13.60)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.41204 |     0.4905 |                    2 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "Result for train_cifar_55ab0_00000:\n",
            "  accuracy: 0.4897\n",
            "  date: 2022-10-16_13-36-18\n",
            "  done: false\n",
            "  experiment_id: 1b6f52b685f341e0b1617944a9a12771\n",
            "  hostname: c45a434aae96\n",
            "  iterations_since_restore: 3\n",
            "  loss: 1.4114297201156616\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 694\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 75.11306428909302\n",
            "  time_this_iter_s: 23.832985639572144\n",
            "  time_total_s: 75.11306428909302\n",
            "  timestamp: 1665927378\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: 55ab0_00000\n",
            "  warmup_time: 0.006904125213623047\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:36:23 (running for 00:01:22.40)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.41143 |     0.4897 |                    3 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:36:28 (running for 00:01:27.42)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.41143 |     0.4897 |                    3 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:36:33 (running for 00:01:32.43)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.41143 |     0.4897 |                    3 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=694)\u001b[0m [4,  2000] loss: 1.386\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:36:38 (running for 00:01:37.44)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.41143 |     0.4897 |                    3 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "Result for train_cifar_55ab0_00000:\n",
            "  accuracy: 0.5318\n",
            "  date: 2022-10-16_13-36-42\n",
            "  done: false\n",
            "  experiment_id: 1b6f52b685f341e0b1617944a9a12771\n",
            "  hostname: c45a434aae96\n",
            "  iterations_since_restore: 4\n",
            "  loss: 1.3334200421333313\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 694\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 98.717458486557\n",
            "  time_this_iter_s: 23.60439419746399\n",
            "  time_total_s: 98.717458486557\n",
            "  timestamp: 1665927402\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 4\n",
            "  trial_id: 55ab0_00000\n",
            "  warmup_time: 0.006904125213623047\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:36:47 (running for 00:01:46.00)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: -1.3334200421333313 | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.33342 |     0.5318 |                    4 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:36:52 (running for 00:01:51.02)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: -1.3334200421333313 | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.33342 |     0.5318 |                    4 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:36:57 (running for 00:01:56.03)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: -1.3334200421333313 | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.33342 |     0.5318 |                    4 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=694)\u001b[0m [5,  2000] loss: 1.352\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:37:02 (running for 00:02:01.05)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: -1.3334200421333313 | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.33342 |     0.5318 |                    4 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "Result for train_cifar_55ab0_00000:\n",
            "  accuracy: 0.5378\n",
            "  date: 2022-10-16_13-37-05\n",
            "  done: false\n",
            "  experiment_id: 1b6f52b685f341e0b1617944a9a12771\n",
            "  hostname: c45a434aae96\n",
            "  iterations_since_restore: 5\n",
            "  loss: 1.3005179386138916\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 694\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 122.2989718914032\n",
            "  time_this_iter_s: 23.58151340484619\n",
            "  time_total_s: 122.2989718914032\n",
            "  timestamp: 1665927425\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: 55ab0_00000\n",
            "  warmup_time: 0.006904125213623047\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:37:10 (running for 00:02:09.58)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: -1.3334200421333313 | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.30052 |     0.5378 |                    5 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:37:15 (running for 00:02:14.60)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: -1.3334200421333313 | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.30052 |     0.5378 |                    5 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:37:20 (running for 00:02:19.61)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: -1.3334200421333313 | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.30052 |     0.5378 |                    5 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-10-16 13:37:21,938\tWARNING tune.py:687 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(func pid=694)\u001b[0m [6,  2000] loss: 1.327\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:37:25 (running for 00:02:24.63)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: -1.3334200421333313 | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.30052 |     0.5378 |                    5 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-10-16 13:37:25 (running for 00:02:24.64)\n",
            "Memory usage on this node: 2.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: -1.3334200421333313 | Iter 2.000: -1.412041555404663 | Iter 1.000: -1.5516036163330078\n",
            "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-10-16_13-35-01\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "| Trial name              | status   | loc            |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
            "|-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
            "| train_cifar_55ab0_00000 | RUNNING  | 172.28.0.2:694 |           16 |    8 |   32 | 0.00814347  | 1.30052 |     0.5378 |                    5 |\n",
            "| train_cifar_55ab0_00001 | PENDING  |                |            4 |    8 |   64 | 0.0139561   |         |            |                      |\n",
            "| train_cifar_55ab0_00002 | PENDING  |                |           16 |  128 |    8 | 0.00227926  |         |            |                      |\n",
            "| train_cifar_55ab0_00003 | PENDING  |                |            2 |  256 |    8 | 0.000314916 |         |            |                      |\n",
            "| train_cifar_55ab0_00004 | PENDING  |                |            8 |    8 |   32 | 0.000722717 |         |            |                      |\n",
            "| train_cifar_55ab0_00005 | PENDING  |                |            2 |  256 |   64 | 0.00180017  |         |            |                      |\n",
            "| train_cifar_55ab0_00006 | PENDING  |                |            8 |  256 |  128 | 0.0311097   |         |            |                      |\n",
            "| train_cifar_55ab0_00007 | PENDING  |                |            2 |  256 |    4 | 0.00209477  |         |            |                      |\n",
            "| train_cifar_55ab0_00008 | PENDING  |                |            4 |    4 |  128 | 0.000346543 |         |            |                      |\n",
            "| train_cifar_55ab0_00009 | PENDING  |                |            8 |    4 |  256 | 0.0432738   |         |            |                      |\n",
            "+-------------------------+----------+----------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8bada6caac0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# You can change the number of GPUs per trial here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-8bada6caac0f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         progress_reporter=reporter)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote)\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0mincomplete_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/execution/trial_runner.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m         \u001b[0;34m\"\"\"Cleanup trials and callbacks.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1468\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_experiment_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/execution/trial_runner.py\u001b[0m in \u001b[0;36mcleanup_trials\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/execution/ray_trial_executor.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(self, trials)\u001b[0m\n\u001b[1;32m    834\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_force_trial_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m             \u001b[0mready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_futures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2465\u001b[0m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2466\u001b[0;31m             \u001b[0mfetch_local\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2467\u001b[0m         )\n\u001b[1;32m   2468\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}